"""LLM Service for Chatbot"""

import os
from typing import Optional, AsyncGenerator

from app.models.chat import ChatMessage, ChatResponse


class LLMService:
    """Large Language Model service for chatbot"""

    def __init__(self):
        self.model = None
        self.tokenizer = None
        self.is_loaded = False
        self._load_model()

    def _load_model(self):
        """Load LLM model"""
        try:
            model_name = os.getenv("LLM_MODEL", "Qwen/Qwen2.5-7B-Instruct")

            # For development/testing, skip loading
            if os.getenv("SKIP_MODEL_LOADING", "false").lower() == "true":
                print("âš ï¸ Skipping LLM model loading (development mode)")
                self.is_loaded = True
                return

            from transformers import AutoModelForCausalLM, AutoTokenizer
            import torch

            print(f"ğŸ“¥ Loading LLM model: {model_name}")

            self.tokenizer = AutoTokenizer.from_pretrained(model_name)
            self.model = AutoModelForCausalLM.from_pretrained(
                model_name,
                torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
                device_map="auto" if torch.cuda.is_available() else None,
            )

            self.is_loaded = True
            print("âœ… LLM model loaded")

        except Exception as e:
            print(f"âŒ Failed to load LLM model: {e}")
            self.is_loaded = False

    async def chat(
        self,
        message: str,
        context: Optional[str] = None,
        history: Optional[list[ChatMessage]] = None,
    ) -> ChatResponse:
        """Generate chat response"""

        # If model not loaded, return mock response
        if self.model is None:
            return self._mock_response(message)

        # Build messages
        messages = self._build_messages(message, context, history)

        # Generate
        text = self.tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True,
        )

        inputs = self.tokenizer(text, return_tensors="pt")
        if self.model.device.type == "cuda":
            inputs = inputs.to("cuda")

        outputs = self.model.generate(
            **inputs,
            max_new_tokens=512,
            do_sample=True,
            temperature=0.7,
            top_p=0.9,
        )

        response_text = self.tokenizer.decode(
            outputs[0][inputs.input_ids.shape[1]:],
            skip_special_tokens=True,
        )

        return ChatResponse(
            message=response_text,
            tokens_used=outputs.shape[1],
        )

    async def chat_stream(
        self,
        message: str,
        context: Optional[str] = None,
        history: Optional[list[ChatMessage]] = None,
    ) -> AsyncGenerator[str, None]:
        """Generate streaming chat response"""

        # If model not loaded, yield mock response
        if self.model is None:
            mock = self._mock_response(message)
            for word in mock.message.split():
                yield word + " "
            return

        # Build messages
        messages = self._build_messages(message, context, history)

        # Generate with streaming
        from transformers import TextIteratorStreamer
        import threading

        text = self.tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True,
        )

        inputs = self.tokenizer(text, return_tensors="pt")
        if self.model.device.type == "cuda":
            inputs = inputs.to("cuda")

        streamer = TextIteratorStreamer(
            self.tokenizer,
            skip_prompt=True,
            skip_special_tokens=True,
        )

        generation_kwargs = {
            **inputs,
            "streamer": streamer,
            "max_new_tokens": 512,
            "do_sample": True,
            "temperature": 0.7,
            "top_p": 0.9,
        }

        thread = threading.Thread(target=self.model.generate, kwargs=generation_kwargs)
        thread.start()

        for text in streamer:
            yield text

    def _build_messages(
        self,
        message: str,
        context: Optional[str],
        history: Optional[list[ChatMessage]],
    ) -> list[dict]:
        """Build message list for chat"""

        system_prompt = """ë‹¹ì‹ ì€ ì—¬í–‰ ì§€ì¶œ ê´€ë¦¬ ì•± 'Travel Helper'ì˜ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.
ì‚¬ìš©ìì˜ ì—¬í–‰ ì§€ì¶œ ê´€ë ¨ ì§ˆë¬¸ì— ì¹œì ˆí•˜ê³  ê°„ê²°í•˜ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”.
í•œêµ­ì–´ë¡œ ë‹µë³€í•˜ë©°, ìˆ«ìëŠ” ì²œ ë‹¨ìœ„ êµ¬ë¶„ì(,)ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
"""

        if context:
            system_prompt += f"\n\nì‚¬ìš©ì ì»¨í…ìŠ¤íŠ¸:\n{context}"

        messages = [{"role": "system", "content": system_prompt}]

        if history:
            for h in history:
                messages.append({"role": h.role, "content": h.content})

        messages.append({"role": "user", "content": message})

        return messages

    def _mock_response(self, message: str) -> ChatResponse:
        """Return mock response for development"""

        # Simple rule-based responses
        if "ì˜¤ëŠ˜" in message and "ì–¼ë§ˆ" in message:
            return ChatResponse(
                message="ì˜¤ëŠ˜ì€ ì´ 45,000ì›ì„ ì‚¬ìš©í•˜ì…¨ì–´ìš”. ì‹ë¹„ 25,000ì›, êµí†µë¹„ 12,000ì›, ê¸°íƒ€ 8,000ì›ì…ë‹ˆë‹¤.",
            )

        if "ì˜ˆì‚°" in message:
            return ChatResponse(
                message="í˜„ì¬ ì˜ˆì‚° ì”ì•¡ì€ 285,000ì›ì´ì—ìš”. ì¼í‰ê·  95,000ì›ì„ ì‚¬ìš©í•˜ê³  ê³„ì‹œë‹ˆ, ì•½ 3ì¼ ì •ë„ ë” ì‚¬ìš©í•˜ì‹¤ ìˆ˜ ìˆì–´ìš”.",
            )

        if "ì¹´í…Œê³ ë¦¬" in message or "ë§ì´" in message:
            return ChatResponse(
                message="ì´ë²ˆ ì—¬í–‰ì—ì„œ ê°€ì¥ ë§ì´ ì§€ì¶œí•œ ì¹´í…Œê³ ë¦¬ëŠ” 'ì‹ë¹„'ë¡œ, ì „ì²´ì˜ 35% (ì•½ 420,000ì›)ë¥¼ ì°¨ì§€í•˜ê³  ìˆì–´ìš”.",
            )

        return ChatResponse(
            message=f"ì•ˆë…•í•˜ì„¸ìš”! ì—¬í–‰ ì§€ì¶œ ê´€ë ¨ ì§ˆë¬¸ì´ ìˆìœ¼ì‹œë©´ í¸í•˜ê²Œ ë¬¼ì–´ë³´ì„¸ìš”. (ëª¨ë¸ ë¯¸ë¡œë“œ ìƒíƒœ)",
        )
